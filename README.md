# A-Survey-on-Mixture-of-Experts


## Algorithm
[[1]](#1).


### Module

#### Experts

##### Dense

##### Sparse

##### Share

##### Placement

##### Activ. Func.

##### Dense

##### Networks



#### Gating Network

##### Dynamic

##### Static




### Training Paradigm

#### Fully Synchronized

#### Asynchronous




### Derivation





## System

### Computation


### Memory


### Communication 





## PEFT


### Attention


### FFN


### Every SubLayer


### Transformer Block





## Application

### NLP


### CV


### RecSys


### MultiModal


## References
<a id="1">[1]</a> DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale	[ICML 2022]




OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER	[ICLR 2017]

Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories	[ACL 2023]

DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models	[ArXiv 2024]

Task-Specific Expert Pruning for Sparse Mixture-of-Experts	[ArXiv 2022]

PANGU-Σ: TOWARDS TRILLION PARAMETER LANGUAGE MODEL WITH SPARSE HETEROGENEOUS COMPUTING	[ArXiv 2023]

Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning	[ArXiv 2023]

SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS	[ArXiv 2023]

Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM	[ArXiv 2024]

Higher Layers Need More LoRA Experts	[ArXiv 2024]

LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin	[ArXiv 2024]

Mixture-of-Depths: Dynamically allocating compute in transformer-based language models	[ArXiv 2024]

MoE-LLaVA: Mixture of Experts for Large Vision-Language Models	[ArXiv 2024]

MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning	[ArXiv 2024]

OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models	[ArXiv 2024]

DENSE-TO-SPARSE GATE FOR MIXTURE-OF-EXPERTS	[ICLR 2022]

FUSING MODELS WITH COMPLEMENTARY EXPERTISE	[ICLR 2024]

BASE Layers: Simplifying Training of Large, Sparse Models	[ICML 2021]

On the Representation Collapse of Sparse Mixture of Experts	[NIPS 2022]

Hash Layers For Large Sparse Models	[NIPS 2021]

Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models	[NIPS 2022]

A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training	[ICS 2023]

A Theoretical View on Sparsely Activated Networks	[ICML 2022]

Go Wider Instead of Deeper	[AAAI 2022]

STABLEMOE: Stable Routing Strategy for Mixture of Experts	[ACL 2022]

Scaling Vision with Sparse Mixture of Experts	[NIPS 2021]

Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning	[ArXiv 2023]

BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores	[PPoPP 2022]

EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models	[ArXiv 2023]

Efficient Large Scale Language Modeling with Mixtures of Experts	[ArXiv 2022]

Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference	[ArXiv 2024]

FasterMoE: Modeling and Optimizing Training of Large-Scale Dynamic Pre-Trained Models	[PPoPP 2022]

FASTMOE: A FAST MIXTURE-OF-EXPERT TRAINING SYSTEM	[ArXiv 2021]

FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement	[2023]

From Sparse to Soft Mixtures of Experts	[ArXiv 2023]

GLaM: Efficient Scaling of Language Models with Mixture-of-Experts	[ICML 2022]

GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding	[ICLR 2021]

Hetu: a highly efficient automatic parallel distributed deep learning system	[2022]

HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System	[ArXiv 2022]

LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs	[ArXiv 2024]

MEGABLOCKS: EFFICIENT SPARSE TRAINING WITH MIXTURE-OF-EXPERTS	[ArXiv 2022]

Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models	[ArXiv 2023]

Mixture-of-Experts with Expert Choice Routing	[NIPS 2022]

MOLE: MIXTURE OF LORA EXPERTS	[ICLR 2024]

MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism	[IPDPS 2023]

Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts	[NIPS 2022]

TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training	[NIPS 2022]

No Language Left Behind: Scaling Human-Centered Machine Translation	[]

Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts	[ArXiv 2023]

Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models	[ArXiv 2022]

Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference	[ArXiv 2023]

Scaling Vision-Language Models with Sparse Mixture of Experts	[EMNLP 2023]

SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization	[ATC 2023]

ST-MOE: DESIGNING STABLE AND TRANSFERABLE SPARSE EXPERT MODELS	[ArXiv 2022]

Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity	[ArXiv 2022]

TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training	[NIPS 2022]

TUTEL: ADAPTIVE MIXTURE-OF-EXPERTS AT SCALE	[MLSys 2023]

UNIFIED SCALING LAWS FOR ROUTED LANGUAGE MODELS	[ArXiv 2022]

Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs	[ArXiv 2022]


